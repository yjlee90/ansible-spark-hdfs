---
- name: Create service account for Spark
  user: name={{ spark.user }}
        system=yes
        shell={{ spark.user_shell }}
        state=present
        groups="{{ spark.user_groups | join(',') }}"

- name: set spark archive fact
  set_fact:
    spark_archive: spark-{{ spark.version }}-bin-hadoop{{ spark.hadoop_version }}.tgz

- name: create install directory
  file:
    path: "{{ spark.install_dir }}"
    state: directory
    owner: "{{ spark.user }}"
    group: "{{ spark.user }}"
  become: true

# Offline
- name: Copy and install spark offline
  become: yes
  unarchive:
    src: "spark-{{ spark.version }}-bin-hadoop2.7.tgz"
    dest: "{{ spark.install_dir }}"
    mode: 0755
    extra_opts: [--strip-components=1]
  when:
    - is_offline


- name: define number of spark workers
  set_fact: number_of_workers="{{ groups['workers'] | length | int }}"


- name: stop spark
  shell: "sbin/stop-all.sh"
  args:
      chdir: "{{ spark.spark_home }}"
  ignore_errors: yes
  when: "{{ spark.reinstall }}"
  


# Online
- block:
  - name: set spark download location fact
    set_fact: 
      spark_download: "{{ spark.download_location }}/spark-{{ spark.version }}/{{ spark_archive }}"
  
  - debug:
       msg: "Downloading Spark from: {{ spark_download }}"
  
  - name: download spark
    get_url:
      url: "{{ spark_download }}"
      dest: "{{ install_temp_dir }}/{{ spark_archive }}"
  
  - name: unarchive to the install directory
    shell: "tar -xvf {{ install_temp_dir }}/{{ spark_archive }} --strip 1 --directory {{ install_dir }}/{{ spark_installation_dir }}"
  when: not is_offline


- name: create spark working directory
  file:
    path: "{{ spark.working_dir }}"
    state: directory
    owner: "{{ spark.user }}"
    group: "{{ spark.user }}"
  become: true

- name: create spark working directory
  file:
    path: "{{ spark.working_dir }}"
    state: directory
    owner: "{{ spark.user }}"
    group: "{{ spark.user }}"
  become: true


- name: set spark-env.sh
  template:
    src: "spark-env-sh.j2"
    dest: "{{ spark.install_dir }}/conf/spark-env.sh"

- name: set spark-defaults.conf
  template: 
    src: "spark-defaults-conf.j2"
    dest: "{{ spark.install_dir }}/conf/spark-defaults.conf"

- name: set slaves
  template:
    src: "slaves.j2"
    dest: "{{ spark.install_dir }}/conf/slaves"

# Environment setup.
- name: add spark profile to startup
  template:
    src: spark-profile.sh.j2
    dest: /etc/profile.d/spark-profile.sh
    mode: 0644

# External jar libaray 
- name: External jar library settings
  copy:
    src: "{{ item }}"
    dest: "{{ spark.spark_home }}/jars"
  with_items:
  - "{{ hadoop.hadoop_home}}/share/hadoop/common/lib/slf4j-api-1.7.25.jar"
  - "{{ hadoop.hadoop_home}}/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar"
